inferenceExtension:
  replicas: 1
  image:
    name: epp
    hub: us-central1-docker.pkg.dev/k8s-staging-images/gateway-api-inference-extension
    tag: main
    pullPolicy: Always
  extProcPort: 9002
  env: []
  pluginsConfigFile: "default-plugins.yaml"
  # Define additional container ports
  extraContainerPorts: []
  # Define additional service ports
  extraServicePorts: []

  # This is the plugins configuration file.
  # pluginsCustomConfig:
  #   custom-plugins.yaml: |
  #     apiVersion: inference.networking.x-k8s.io/v1alpha1
  #     kind: EndpointPickerConfig
  #     plugins:
  #     - type: custom-scorer
  #       parameters:
  #         custom-threshold: 64
  #     schedulingProfiles:
  #     - name: default
  #       plugins:
  #       - pluginRef: custom-scorer

  # Example environment variables:
  # env:
  #   ENABLE_EXPERIMENTAL_FEATURE: "true"

  flags:
    # Log verbosity
    - name: v
      value: 1

  # Monitoring configuration for EPP
  monitoring:
    # Common monitoring parameters
    path: "/metrics"
    interval: "10s"
    scheme: "http"
    # port -- Port name to scrape metrics from (must match service port name)
    port: "http-metrics"
    # secret -- Service account token secret for authentication
    secret:
      name: inference-gateway-sa-metrics-reader-secret

    # Prometheus ServiceMonitor configuration for EPP metrics collection with Prometheus Operator
    prometheus:
      enabled: false
      # scrapeTimeout: "10s"
      # relabelings -- RelabelConfigs to apply to samples before scraping
      relabelings: []
      # metricRelabelings -- MetricRelabelConfigs to apply to samples before ingestion
      metricRelabelings: []
      namespaceSelector:
        any: false
        matchNames: []
      selector:
        # matchLabels in template match the EPP service by default
        matchLabels: {}

    # GKE monitoring configuration (ClusterPodMonitoring)
    gke:
      enabled: false

inferencePool:
  targetPorts:
    - number: 8000
  modelServerType: vllm # vllm, triton-tensorrt-llm
  modelServers:
    matchLabels:
      app: vllm-llama3-8b-instruct

provider:
  name: none


inferenceExtension:
  replicas: 1
  image:
    name: epp
    hub: us-central1-docker.pkg.dev/k8s-staging-images/gateway-api-inference-extension
    tag: main
    pullPolicy: Always
  extProcPort: 9002
  env: []
  enablePprof: true # Enable pprof handlers for profiling and debugging
  modelServerMetricsPath: "/metrics"
  modelServerMetricsScheme: "http"
  modelServerMetricsHttpsInsecureSkipVerify: true
  # This is the plugins configuration file. 
  pluginsConfigFile: "default-plugins.yaml"
  # pluginsCustomConfig:
  #   custom-plugins.yaml: |
  #     apiVersion: inference.networking.x-k8s.io/v1alpha1
  #     kind: EndpointPickerConfig
  #     plugins:
  #     - type: custom-scorer
  #       parameters:
  #         custom-threshold: 64
  #     schedulingProfiles:
  #     - name: default
  #       plugins:
  #       - pluginRef: custom-scorer

  # Example environment variables:
  # env:
  #   KV_CACHE_SCORE_WEIGHT: "1"

  # Define additional container ports
  extraContainerPorts: []
  # Define additional service ports
  extraServicePorts: []
  # Enable leader election for high availability. When enabled, it is recommended to set replicas > 1.
  # Only the leader pod will be ready to serve traffic.
  enableLeaderElection: false

  # Monitoring configuration
  metricsPort: 9090
  monitoring:
    servicemonitor:
      enabled: false
      port: "http-metrics"
      path: "/metrics"
      interval: "30s"
      scrapeTimeout: ""
      labels: {}
      annotations: {}
      relabelings: []
      metricRelabelings: []
      namespaceSelector:
        any: false
        matchNames: []
      selector:
        matchLabels: {} # servicemonitor matchLabels will match epp service by default.

inferencePool:
  targetPortNumber: 8000
  modelServerType: vllm # vllm, triton-tensorrt-llm
  # modelServers: # REQUIRED
    # matchLabels: 
    #   app: vllm-llama3-8b-instruct

provider:
  name: none

gke:
  monitoringSecret:
    name: inference-gateway-sa-metrics-reader-secret
    namespace: default
